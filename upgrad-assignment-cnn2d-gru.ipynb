{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Gesture Recognition\nIn this group project, you are going to build a 3D Conv model that will be able to predict the 5 gestures correctly. Please import the following libraries to get started.","metadata":{}},{"cell_type":"code","source":"!pip3 install scipy==1.2\n!pip3 install gdown\n!gdown https://drive.google.com/uc?id=1ehyrYBQ5rbQQe6yL4XbLWe3FMvuVUGiL\n!unzip -q Project_data.zip","metadata":{"execution":{"iopub.status.busy":"2022-02-23T15:52:25.572425Z","iopub.execute_input":"2022-02-23T15:52:25.572760Z","iopub.status.idle":"2022-02-23T15:53:27.400968Z","shell.execute_reply.started":"2022-02-23T15:52:25.572676Z","shell.execute_reply":"2022-02-23T15:53:27.399855Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting scipy==1.2\n  Downloading scipy-1.2.0-cp37-cp37m-manylinux1_x86_64.whl (26.6 MB)\n     |████████████████████████████████| 26.6 MB 4.2 MB/s            \n\u001b[?25hRequirement already satisfied: numpy>=1.8.2 in /opt/conda/lib/python3.7/site-packages (from scipy==1.2) (1.20.3)\nInstalling collected packages: scipy\n  Attempting uninstall: scipy\n    Found existing installation: scipy 1.7.3\n    Uninstalling scipy-1.7.3:\n      Successfully uninstalled scipy-1.7.3\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nyellowbrick 1.3.post1 requires numpy<1.20,>=1.16.0, but you have numpy 1.20.3 which is incompatible.\ntpot 0.11.7 requires scipy>=1.3.1, but you have scipy 1.2.0 which is incompatible.\nstumpy 1.10.2 requires scipy>=1.5, but you have scipy 1.2.0 which is incompatible.\nstatsmodels 0.13.1 requires scipy>=1.3, but you have scipy 1.2.0 which is incompatible.\nsklearn-pandas 2.2.0 requires scipy>=1.5.1, but you have scipy 1.2.0 which is incompatible.\nscikit-image 0.19.1 requires scipy>=1.4.1, but you have scipy 1.2.0 which is incompatible.\nplotnine 0.8.0 requires scipy>=1.5.0, but you have scipy 1.2.0 which is incompatible.\nphik 0.12.0 requires scipy>=1.5.2, but you have scipy 1.2.0 which is incompatible.\npdpbox 0.2.1 requires matplotlib==3.1.1, but you have matplotlib 3.5.1 which is incompatible.\npandas-profiling 3.0.0 requires scipy>=1.4.1, but you have scipy 1.2.0 which is incompatible.\nmlxtend 0.19.0 requires scipy>=1.2.1, but you have scipy 1.2.0 which is incompatible.\nmatrixprofile 1.1.10 requires scipy<2.0.0,>=1.3.2, but you have scipy 1.2.0 which is incompatible.\njax 0.2.28 requires scipy>=1.2.1, but you have scipy 1.2.0 which is incompatible.\nimbalanced-learn 0.9.0 requires scikit-learn>=1.0.1, but you have scikit-learn 0.23.2 which is incompatible.\ngiddy 2.3.3 requires scipy>=1.3.0, but you have scipy 1.2.0 which is incompatible.\nfeaturetools 1.4.1 requires numpy>=1.21.0, but you have numpy 1.20.3 which is incompatible.\nfeaturetools 1.4.1 requires scipy>=1.3.3, but you have scipy 1.2.0 which is incompatible.\narviz 0.11.4 requires typing-extensions<4,>=3.7.4.3, but you have typing-extensions 4.0.1 which is incompatible.\u001b[0m\nSuccessfully installed scipy-1.2.0\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\nCollecting gdown\n  Downloading gdown-4.3.1.tar.gz (13 kB)\n  Installing build dependencies ... \u001b[?25ldone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from gdown) (4.62.3)\nRequirement already satisfied: beautifulsoup4 in /opt/conda/lib/python3.7/site-packages (from gdown) (4.10.0)\nRequirement already satisfied: requests[socks] in /opt/conda/lib/python3.7/site-packages (from gdown) (2.26.0)\nRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from gdown) (1.16.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from gdown) (3.4.2)\nRequirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.7/site-packages (from beautifulsoup4->gdown) (2.3.1)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests[socks]->gdown) (2021.10.8)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests[socks]->gdown) (3.1)\nRequirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.7/site-packages (from requests[socks]->gdown) (2.0.9)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests[socks]->gdown) (1.26.7)\nRequirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /opt/conda/lib/python3.7/site-packages (from requests[socks]->gdown) (1.7.1)\nBuilding wheels for collected packages: gdown\n  Building wheel for gdown (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for gdown: filename=gdown-4.3.1-py3-none-any.whl size=14493 sha256=341de4a71126a1dfd9806582ea01156dfb1ada51a41d7781745f97ae4975474c\n  Stored in directory: /root/.cache/pip/wheels/39/13/56/88209f07bace2c1af0614ee3326de4a00aad74afb0f4be921d\nSuccessfully built gdown\nInstalling collected packages: gdown\nSuccessfully installed gdown-4.3.1\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\nDownloading...\nFrom: https://drive.google.com/uc?id=1ehyrYBQ5rbQQe6yL4XbLWe3FMvuVUGiL\nTo: /kaggle/working/Project_data.zip\n100%|███████████████████████████████████████| 1.71G/1.71G [00:06<00:00, 279MB/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np\nimport os\nfrom scipy.misc import imread, imresize\nimport datetime\nimport os\nimport cv2","metadata":{"execution":{"iopub.status.busy":"2022-02-23T15:53:27.403333Z","iopub.execute_input":"2022-02-23T15:53:27.403616Z","iopub.status.idle":"2022-02-23T15:53:28.069227Z","shell.execute_reply.started":"2022-02-23T15:53:27.403581Z","shell.execute_reply":"2022-02-23T15:53:28.068420Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"We set the random seed so that the results don't vary drastically.","metadata":{}},{"cell_type":"code","source":"np.random.seed(30)\nimport random as rn\nrn.seed(30)\nfrom keras import backend as K\nimport tensorflow as tf\ntf.random.set_seed(30)","metadata":{"execution":{"iopub.status.busy":"2022-02-23T15:53:28.070492Z","iopub.execute_input":"2022-02-23T15:53:28.070938Z","iopub.status.idle":"2022-02-23T15:53:33.642119Z","shell.execute_reply.started":"2022-02-23T15:53:28.070899Z","shell.execute_reply":"2022-02-23T15:53:33.641198Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"In this block, you read the folder names for training and validation. You also set the `batch_size` here. Note that you set the batch size in such a way that you are able to use the GPU in full capacity. You keep increasing the batch size until the machine throws an error.","metadata":{}},{"cell_type":"code","source":"\ntrain_doc = np.random.permutation(open('Project_data/train.csv').readlines())\nval_doc = np.random.permutation(open('Project_data/val.csv').readlines())\nbatch_size = 32 #experiment with the batch size","metadata":{"execution":{"iopub.status.busy":"2022-02-23T15:53:33.643711Z","iopub.execute_input":"2022-02-23T15:53:33.644235Z","iopub.status.idle":"2022-02-23T15:53:33.655578Z","shell.execute_reply.started":"2022-02-23T15:53:33.644179Z","shell.execute_reply":"2022-02-23T15:53:33.653737Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"## Generator\nThis is one of the most important part of the code. The overall structure of the generator has been given. In the generator, you are going to preprocess the images as you have images of 2 different dimensions as well as create a batch of video frames. You have to experiment with `img_idx`, `y`,`z` and normalization such that you get high accuracy.","metadata":{}},{"cell_type":"code","source":"def generator(source_path, folder_list, batch_size):\n    print( 'Source path = ', source_path, '; batch size =', batch_size)\n    img_idx = [0,1,2,6,10,14,16,20,24,27,28,29]\n#create a list of image numbers you want to use for a particular video\n    while True:\n        t = np.random.permutation(folder_list)\n        num_batches = int(len(folder_list)/batch_size)\n        # calculate the number of batches\n        for batch in range(num_batches): # we iterate over the number of batches\n            x=len(img_idx)\n            #batch_data = np.zeros((batch_size,x,y,z,3)) # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n            batch_data = np.zeros((batch_size,x,100,100,3))\n            batch_labels = np.zeros((batch_size,5)) # batch_labels is the one hot representation of the output\n            for folder in range(batch_size): # iterate over the batch_size\n                imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0]) # read all the images in the folder\n                for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in\n                    image = imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n                     \n                    #cropping an image if size height or width is 120* 160\n                    height, width , channel = image.shape\n                    if height == 120 or width == 120:\n                        image=image[20:140,:120,:]\n                    #crop the images and resize them. Note that the images are of 2 different shape                     \n                    #and the conv3D will throw error if the inputs in a batch have different shapes\n                    #resizing all iamge to 100*100\n                    image = cv2.resize(image,(100,100))\n                    image = image/255\n                    batch_data[folder,idx,:,:,0] = image[:,:,0]\n                    #normalise and feed in the image\n                    batch_data[folder,idx,:,:,1] = image[:,:,1]\n                    #normalise and feed in the image\n                    batch_data[folder,idx,:,:,2] = image[:,:,2]\n                    #normalise and feed in the image\n                \n                batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n            yield batch_data, batch_labels #you yield the batch_data and the batch_labels, remember what does yield do\n        \n        # write the code for the remaining data points which are left after full batches\n        if len(folder_list) > num_batches * batch_size:\n            x=len(img_idx)\n            remaining_data_size = len(folder_list) - (num_batches * batch_size)\n            batch_data = np.zeros((remaining_data_size,x,100,100,3))\n            batch_labels = np.zeros((remaining_data_size,5)) # batch_labels is the one hot representation of the output\n            for folder in range(remaining_data_size): # iterate over the batch_size\n                imgs = os.listdir(source_path+'/'+ t[folder + (num_batches * batch_size)].split(';')[0]) # read all the images in the folder\n                for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in\n                    image = imread(source_path+'/'+ t[folder + (num_batches * batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n                     \n                    #cropping an image if size height or width is 120* 160\n                    height, width , channel = image.shape\n                    if height == 120 or width == 120:\n                        image=image[20:140,:120,:]\n                    #crop the images and resize them. Note that the images are of 2 different shape                     \n                    #and the conv3D will throw error if the inputs in a batch have different shapes\n                    #resizing all iamge to 100*100\n                    image = cv2.resize(image,(100,100))\n                    image = image/255\n                    batch_data[folder,idx,:,:,0] = image[:,:,0]\n                    #normalise and feed in the image\n                    batch_data[folder,idx,:,:,1] = image[:,:,1]\n                    #normalise and feed in the image\n                    batch_data[folder,idx,:,:,2] = image[:,:,2]\n                    #normalise and feed in the image\n                \n                batch_labels[folder, int(t[folder + (num_batches * batch_size)].strip().split(';')[2])] = 1\n            yield batch_data, batch_labels #you yield the batch_data and the batch_labels, remember what does yield do            \n","metadata":{"execution":{"iopub.status.busy":"2022-02-23T15:53:33.660915Z","iopub.execute_input":"2022-02-23T15:53:33.661197Z","iopub.status.idle":"2022-02-23T15:53:33.690757Z","shell.execute_reply.started":"2022-02-23T15:53:33.661160Z","shell.execute_reply":"2022-02-23T15:53:33.689737Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"Note here that a video is represented above in the generator as (number of images, height, width, number of channels). Take this into consideration while creating the model architecture.","metadata":{}},{"cell_type":"code","source":"curr_dt_time = datetime.datetime.now()\ntrain_path = 'Project_data/train'\nval_path = 'Project_data/val'\nnum_train_sequences = len(train_doc)\nprint('# training sequences =', num_train_sequences)\nnum_val_sequences = len(val_doc)\nprint('# validation sequences =', num_val_sequences)\nnum_epochs = 75 # choose the number of epochs\nprint ('# epochs =', num_epochs)","metadata":{"execution":{"iopub.status.busy":"2022-02-23T15:53:33.693974Z","iopub.execute_input":"2022-02-23T15:53:33.694200Z","iopub.status.idle":"2022-02-23T15:53:35.031139Z","shell.execute_reply.started":"2022-02-23T15:53:33.694173Z","shell.execute_reply":"2022-02-23T15:53:35.030242Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"# training sequences = 663\n# validation sequences = 100\n# epochs = 75\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Model\nHere you make the model using different functionalities that Keras provides. Remember to use `Conv3D` and `MaxPooling3D` and not `Conv2D` and `Maxpooling2D` for a 3D convolution model. You would want to use `TimeDistributed` while building a Conv2D + RNN model. Also remember that the last layer is the softmax. Design the network in such a way that the model is able to give good accuracy on the least number of parameters so that it can fit in the memory of the webcam.","metadata":{}},{"cell_type":"code","source":"from keras.models import Sequential, Model\nfrom keras.layers import Dense, GRU, Flatten, TimeDistributed, Flatten, BatchNormalization, Activation, Dropout\nfrom keras.layers.convolutional import Conv2D, MaxPooling2D\nfrom keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\nfrom keras import optimizers\n#Our import\nfrom keras.regularizers import l2\n\n#write your model here\ncnn = Sequential()\ncnn.add(Conv2D(32, (3, 3), padding='same', input_shape=(100,100,3)))\ncnn.add(BatchNormalization())\ncnn.add(Activation('relu'))\ncnn.add(MaxPooling2D(pool_size=(2,2)))\n\ncnn.add(Conv2D(64, (3, 3), padding='same', input_shape=(100,100,3)))\ncnn.add(BatchNormalization())\ncnn.add(Activation('relu'))\ncnn.add(MaxPooling2D(pool_size=(2,2)))\n\ncnn.add(Conv2D(128, (3, 3), padding='same', input_shape=(100,100,3)))\ncnn.add(BatchNormalization())\ncnn.add(Activation('relu'))\ncnn.add(MaxPooling2D(pool_size=(2,2)))\n\ncnn.add(Conv2D(128, (3, 3), padding='same', input_shape=(100,100,3)))\ncnn.add(BatchNormalization())\ncnn.add(Activation('relu')) \ncnn.add(MaxPooling2D(pool_size=(2,2)))\ncnn.add(Dropout(0.25))\n\ncnn.add(Conv2D(128, (3, 3), padding='same', input_shape=(100,100,3)))\ncnn.add(BatchNormalization())\ncnn.add(Activation('relu')) \ncnn.add(MaxPooling2D(pool_size=(2,2)))\ncnn.add(Dropout(0.25))\n\n\n#model.add(LSTM(100,return_sequences=True))\ncnn.add(Flatten())\ncnn.add(Dense(256, activation='relu'))\ncnn.add(Dropout(0.5))\ncnn.add(Dense(256, activation='relu'))\nmodel = Sequential()\nmodel.add(TimeDistributed(cnn, input_shape=(12, 100, 100,3)))\nmodel.add(GRU(12))\nmodel.add(Dropout(.2)) #added\nmodel.add(Dense(5, activation='softmax'))","metadata":{"execution":{"iopub.status.busy":"2022-02-23T15:53:35.032508Z","iopub.execute_input":"2022-02-23T15:53:35.033125Z","iopub.status.idle":"2022-02-23T15:53:39.026084Z","shell.execute_reply.started":"2022-02-23T15:53:35.033088Z","shell.execute_reply":"2022-02-23T15:53:39.025263Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stderr","text":"2022-02-23 15:53:36.305129: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-02-23 15:53:36.451981: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-02-23 15:53:36.452899: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-02-23 15:53:36.454309: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2022-02-23 15:53:36.455218: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-02-23 15:53:36.455945: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-02-23 15:53:36.456666: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-02-23 15:53:38.183974: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-02-23 15:53:38.184823: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-02-23 15:53:38.185466: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-02-23 15:53:38.186098: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15403 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Now that you have written the model, the next step is to `compile` the model. When you print the `summary` of the model, you'll see the total number of parameters you have to train.","metadata":{}},{"cell_type":"code","source":"optimiser = tf.keras.optimizers.Adam()\nmodel.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\nprint (model.summary())","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-02-23T15:53:39.027474Z","iopub.execute_input":"2022-02-23T15:53:39.027700Z","iopub.status.idle":"2022-02-23T15:53:39.369830Z","shell.execute_reply.started":"2022-02-23T15:53:39.027669Z","shell.execute_reply":"2022-02-23T15:53:39.369104Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Model: \"sequential_1\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ntime_distributed (TimeDistri (None, 12, 256)           751296    \n_________________________________________________________________\ngru (GRU)                    (None, 12)                9720      \n_________________________________________________________________\ndropout_3 (Dropout)          (None, 12)                0         \n_________________________________________________________________\ndense_2 (Dense)              (None, 5)                 65        \n=================================================================\nTotal params: 761,081\nTrainable params: 760,121\nNon-trainable params: 960\n_________________________________________________________________\nNone\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Let us create the `train_generator` and the `val_generator` which will be used in `.fit_generator`.","metadata":{}},{"cell_type":"code","source":"train_generator = generator(train_path, train_doc, batch_size)\nval_generator = generator(val_path, val_doc, batch_size)","metadata":{"execution":{"iopub.status.busy":"2022-02-23T15:53:39.371257Z","iopub.execute_input":"2022-02-23T15:53:39.371745Z","iopub.status.idle":"2022-02-23T15:53:39.376802Z","shell.execute_reply.started":"2022-02-23T15:53:39.371698Z","shell.execute_reply":"2022-02-23T15:53:39.375873Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"model_name = 'model_init' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n    \nif not os.path.exists(model_name):\n    os.mkdir(model_name)\n        \nfilepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n\ncheckpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n\nLR = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=10, min_lr=0.0001) # write the REducelronplateau code here\n#try different patience level\ncallbacks_list = [checkpoint, LR]","metadata":{"execution":{"iopub.status.busy":"2022-02-23T15:53:39.378923Z","iopub.execute_input":"2022-02-23T15:53:39.380565Z","iopub.status.idle":"2022-02-23T15:53:39.394846Z","shell.execute_reply.started":"2022-02-23T15:53:39.380517Z","shell.execute_reply":"2022-02-23T15:53:39.393791Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"The `steps_per_epoch` and `validation_steps` are used by `fit_generator` to decide the number of next() calls it need to make.","metadata":{}},{"cell_type":"code","source":"if (num_train_sequences%batch_size) == 0:\n    steps_per_epoch = int(num_train_sequences/batch_size)\nelse:\n    steps_per_epoch = (num_train_sequences//batch_size) + 1\n\nif (num_val_sequences%batch_size) == 0:\n    validation_steps = int(num_val_sequences/batch_size)\nelse:\n    validation_steps = (num_val_sequences//batch_size) + 1","metadata":{"execution":{"iopub.status.busy":"2022-02-23T15:53:39.397130Z","iopub.execute_input":"2022-02-23T15:53:39.398370Z","iopub.status.idle":"2022-02-23T15:53:39.407839Z","shell.execute_reply.started":"2022-02-23T15:53:39.398326Z","shell.execute_reply":"2022-02-23T15:53:39.406886Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"Let us now fit the model. This will start training the model and with the help of the checkpoints, you'll be able to save the model at the end of each epoch.","metadata":{}},{"cell_type":"code","source":"model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n                    callbacks=callbacks_list, validation_data=val_generator, \n                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)","metadata":{"execution":{"iopub.status.busy":"2022-02-23T15:53:39.409342Z","iopub.execute_input":"2022-02-23T15:53:39.409892Z","iopub.status.idle":"2022-02-23T16:31:52.344348Z","shell.execute_reply.started":"2022-02-23T15:53:39.409856Z","shell.execute_reply":"2022-02-23T16:31:52.343604Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"Source path =  Project_data/train ; batch size = 32\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/keras/engine/training.py:1972: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n  warnings.warn('`Model.fit_generator` is deprecated and '\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:17: DeprecationWarning:     `imread` is deprecated!\n    `imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n    Use ``imageio.imread`` instead.\n2022-02-23 15:53:40.635229: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/75\n","output_type":"stream"},{"name":"stderr","text":"2022-02-23 15:53:43.989923: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8005\n","output_type":"stream"},{"name":"stdout","text":"20/21 [===========================>..] - ETA: 1s - loss: 1.5477 - categorical_accuracy: 0.3281","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:47: DeprecationWarning:     `imread` is deprecated!\n    `imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n    Use ``imageio.imread`` instead.\n","output_type":"stream"},{"name":"stdout","text":"21/21 [==============================] - ETA: 0s - loss: 1.5412 - categorical_accuracy: 0.3303Source path =  Project_data/val ; batch size = 32\n21/21 [==============================] - 40s 2s/step - loss: 1.5412 - categorical_accuracy: 0.3303 - val_loss: 2.4006 - val_categorical_accuracy: 0.2100\n\nEpoch 00001: saving model to model_init_2022-02-2315_53_35.026342/model-00001-1.54125-0.33032-2.40055-0.21000.h5\nEpoch 2/75\n21/21 [==============================] - 31s 2s/step - loss: 1.3591 - categorical_accuracy: 0.3846 - val_loss: 2.1239 - val_categorical_accuracy: 0.2200\n\nEpoch 00002: saving model to model_init_2022-02-2315_53_35.026342/model-00002-1.35911-0.38462-2.12394-0.22000.h5\nEpoch 3/75\n21/21 [==============================] - 31s 2s/step - loss: 1.2473 - categorical_accuracy: 0.4630 - val_loss: 2.4044 - val_categorical_accuracy: 0.2000\n\nEpoch 00003: saving model to model_init_2022-02-2315_53_35.026342/model-00003-1.24733-0.46305-2.40442-0.20000.h5\nEpoch 4/75\n21/21 [==============================] - 29s 1s/step - loss: 1.1796 - categorical_accuracy: 0.5008 - val_loss: 2.2402 - val_categorical_accuracy: 0.2100\n\nEpoch 00004: saving model to model_init_2022-02-2315_53_35.026342/model-00004-1.17958-0.50075-2.24020-0.21000.h5\nEpoch 5/75\n21/21 [==============================] - 31s 2s/step - loss: 1.1994 - categorical_accuracy: 0.4917 - val_loss: 2.1806 - val_categorical_accuracy: 0.2300\n\nEpoch 00005: saving model to model_init_2022-02-2315_53_35.026342/model-00005-1.19936-0.49170-2.18064-0.23000.h5\nEpoch 6/75\n21/21 [==============================] - 31s 2s/step - loss: 1.1208 - categorical_accuracy: 0.5385 - val_loss: 2.1709 - val_categorical_accuracy: 0.2500\n\nEpoch 00006: saving model to model_init_2022-02-2315_53_35.026342/model-00006-1.12075-0.53846-2.17087-0.25000.h5\nEpoch 7/75\n21/21 [==============================] - 31s 2s/step - loss: 1.0581 - categorical_accuracy: 0.5460 - val_loss: 2.2177 - val_categorical_accuracy: 0.2400\n\nEpoch 00007: saving model to model_init_2022-02-2315_53_35.026342/model-00007-1.05810-0.54600-2.21769-0.24000.h5\nEpoch 8/75\n21/21 [==============================] - 30s 1s/step - loss: 0.9869 - categorical_accuracy: 0.6154 - val_loss: 2.2647 - val_categorical_accuracy: 0.2200\n\nEpoch 00008: saving model to model_init_2022-02-2315_53_35.026342/model-00008-0.98687-0.61538-2.26473-0.22000.h5\nEpoch 9/75\n21/21 [==============================] - 31s 2s/step - loss: 1.0038 - categorical_accuracy: 0.6275 - val_loss: 2.2727 - val_categorical_accuracy: 0.2400\n\nEpoch 00009: saving model to model_init_2022-02-2315_53_35.026342/model-00009-1.00375-0.62745-2.27271-0.24000.h5\nEpoch 10/75\n21/21 [==============================] - 31s 2s/step - loss: 0.9020 - categorical_accuracy: 0.6621 - val_loss: 1.7142 - val_categorical_accuracy: 0.4000\n\nEpoch 00010: saving model to model_init_2022-02-2315_53_35.026342/model-00010-0.90204-0.66214-1.71421-0.40000.h5\nEpoch 11/75\n21/21 [==============================] - 31s 2s/step - loss: 0.8302 - categorical_accuracy: 0.7074 - val_loss: 1.6188 - val_categorical_accuracy: 0.3700\n\nEpoch 00011: saving model to model_init_2022-02-2315_53_35.026342/model-00011-0.83020-0.70739-1.61880-0.37000.h5\nEpoch 12/75\n21/21 [==============================] - 30s 1s/step - loss: 0.7863 - categorical_accuracy: 0.7255 - val_loss: 1.3072 - val_categorical_accuracy: 0.4500\n\nEpoch 00012: saving model to model_init_2022-02-2315_53_35.026342/model-00012-0.78631-0.72549-1.30715-0.45000.h5\nEpoch 13/75\n21/21 [==============================] - 30s 2s/step - loss: 0.8172 - categorical_accuracy: 0.7285 - val_loss: 0.9732 - val_categorical_accuracy: 0.6000\n\nEpoch 00013: saving model to model_init_2022-02-2315_53_35.026342/model-00013-0.81722-0.72851-0.97321-0.60000.h5\nEpoch 14/75\n21/21 [==============================] - 31s 2s/step - loss: 0.7208 - categorical_accuracy: 0.7707 - val_loss: 0.9385 - val_categorical_accuracy: 0.6600\n\nEpoch 00014: saving model to model_init_2022-02-2315_53_35.026342/model-00014-0.72075-0.77074-0.93850-0.66000.h5\nEpoch 15/75\n21/21 [==============================] - 31s 2s/step - loss: 0.6648 - categorical_accuracy: 0.7903 - val_loss: 1.3006 - val_categorical_accuracy: 0.5200\n\nEpoch 00015: saving model to model_init_2022-02-2315_53_35.026342/model-00015-0.66481-0.79035-1.30062-0.52000.h5\nEpoch 16/75\n21/21 [==============================] - 30s 1s/step - loss: 0.6350 - categorical_accuracy: 0.7979 - val_loss: 0.8116 - val_categorical_accuracy: 0.7100\n\nEpoch 00016: saving model to model_init_2022-02-2315_53_35.026342/model-00016-0.63500-0.79789-0.81164-0.71000.h5\nEpoch 17/75\n21/21 [==============================] - 31s 2s/step - loss: 0.6065 - categorical_accuracy: 0.8220 - val_loss: 1.1128 - val_categorical_accuracy: 0.5400\n\nEpoch 00017: saving model to model_init_2022-02-2315_53_35.026342/model-00017-0.60645-0.82202-1.11284-0.54000.h5\nEpoch 18/75\n21/21 [==============================] - 31s 2s/step - loss: 0.6367 - categorical_accuracy: 0.7994 - val_loss: 0.9003 - val_categorical_accuracy: 0.7100\n\nEpoch 00018: saving model to model_init_2022-02-2315_53_35.026342/model-00018-0.63667-0.79940-0.90033-0.71000.h5\nEpoch 19/75\n21/21 [==============================] - 31s 2s/step - loss: 0.5128 - categorical_accuracy: 0.8522 - val_loss: 1.0835 - val_categorical_accuracy: 0.5400\n\nEpoch 00019: saving model to model_init_2022-02-2315_53_35.026342/model-00019-0.51282-0.85219-1.08352-0.54000.h5\nEpoch 20/75\n21/21 [==============================] - 30s 1s/step - loss: 0.4930 - categorical_accuracy: 0.8658 - val_loss: 1.0316 - val_categorical_accuracy: 0.5900\n\nEpoch 00020: saving model to model_init_2022-02-2315_53_35.026342/model-00020-0.49301-0.86576-1.03161-0.59000.h5\nEpoch 21/75\n21/21 [==============================] - 31s 2s/step - loss: 0.4499 - categorical_accuracy: 0.8733 - val_loss: 0.8775 - val_categorical_accuracy: 0.6500\n\nEpoch 00021: saving model to model_init_2022-02-2315_53_35.026342/model-00021-0.44985-0.87330-0.87754-0.65000.h5\nEpoch 22/75\n21/21 [==============================] - 31s 2s/step - loss: 0.4493 - categorical_accuracy: 0.8703 - val_loss: 0.6693 - val_categorical_accuracy: 0.7500\n\nEpoch 00022: saving model to model_init_2022-02-2315_53_35.026342/model-00022-0.44931-0.87029-0.66931-0.75000.h5\nEpoch 23/75\n21/21 [==============================] - 31s 2s/step - loss: 0.4279 - categorical_accuracy: 0.8839 - val_loss: 1.5640 - val_categorical_accuracy: 0.5400\n\nEpoch 00023: saving model to model_init_2022-02-2315_53_35.026342/model-00023-0.42791-0.88386-1.56400-0.54000.h5\nEpoch 24/75\n21/21 [==============================] - 30s 1s/step - loss: 0.4624 - categorical_accuracy: 0.8748 - val_loss: 0.5633 - val_categorical_accuracy: 0.8200\n\nEpoch 00024: saving model to model_init_2022-02-2315_53_35.026342/model-00024-0.46243-0.87481-0.56326-0.82000.h5\nEpoch 25/75\n21/21 [==============================] - 31s 2s/step - loss: 0.5050 - categorical_accuracy: 0.8386 - val_loss: 0.8256 - val_categorical_accuracy: 0.7100\n\nEpoch 00025: saving model to model_init_2022-02-2315_53_35.026342/model-00025-0.50504-0.83861-0.82561-0.71000.h5\nEpoch 26/75\n21/21 [==============================] - 31s 2s/step - loss: 0.3850 - categorical_accuracy: 0.9005 - val_loss: 0.8866 - val_categorical_accuracy: 0.7000\n\nEpoch 00026: saving model to model_init_2022-02-2315_53_35.026342/model-00026-0.38497-0.90045-0.88661-0.70000.h5\nEpoch 27/75\n21/21 [==============================] - 31s 2s/step - loss: 0.3419 - categorical_accuracy: 0.9186 - val_loss: 0.9252 - val_categorical_accuracy: 0.7200\n\nEpoch 00027: saving model to model_init_2022-02-2315_53_35.026342/model-00027-0.34186-0.91855-0.92518-0.72000.h5\nEpoch 28/75\n21/21 [==============================] - 30s 1s/step - loss: 0.2894 - categorical_accuracy: 0.9442 - val_loss: 0.8890 - val_categorical_accuracy: 0.6700\n\nEpoch 00028: saving model to model_init_2022-02-2315_53_35.026342/model-00028-0.28936-0.94419-0.88895-0.67000.h5\nEpoch 29/75\n21/21 [==============================] - 31s 2s/step - loss: 0.2328 - categorical_accuracy: 0.9653 - val_loss: 0.6586 - val_categorical_accuracy: 0.8000\n\nEpoch 00029: saving model to model_init_2022-02-2315_53_35.026342/model-00029-0.23279-0.96531-0.65863-0.80000.h5\nEpoch 30/75\n21/21 [==============================] - 31s 2s/step - loss: 0.2261 - categorical_accuracy: 0.9638 - val_loss: 0.9105 - val_categorical_accuracy: 0.6500\n\nEpoch 00030: saving model to model_init_2022-02-2315_53_35.026342/model-00030-0.22613-0.96380-0.91046-0.65000.h5\nEpoch 31/75\n21/21 [==============================] - 31s 2s/step - loss: 0.1997 - categorical_accuracy: 0.9744 - val_loss: 0.7352 - val_categorical_accuracy: 0.7300\n\nEpoch 00031: saving model to model_init_2022-02-2315_53_35.026342/model-00031-0.19973-0.97436-0.73517-0.73000.h5\nEpoch 32/75\n21/21 [==============================] - 30s 1s/step - loss: 0.2055 - categorical_accuracy: 0.9608 - val_loss: 0.7756 - val_categorical_accuracy: 0.7600\n\nEpoch 00032: saving model to model_init_2022-02-2315_53_35.026342/model-00032-0.20553-0.96078-0.77564-0.76000.h5\nEpoch 33/75\n21/21 [==============================] - 30s 2s/step - loss: 0.1957 - categorical_accuracy: 0.9668 - val_loss: 0.6478 - val_categorical_accuracy: 0.8200\n\nEpoch 00033: saving model to model_init_2022-02-2315_53_35.026342/model-00033-0.19573-0.96682-0.64778-0.82000.h5\nEpoch 34/75\n21/21 [==============================] - 30s 2s/step - loss: 0.1486 - categorical_accuracy: 0.9894 - val_loss: 0.5148 - val_categorical_accuracy: 0.8100\n\nEpoch 00034: saving model to model_init_2022-02-2315_53_35.026342/model-00034-0.14865-0.98944-0.51475-0.81000.h5\nEpoch 35/75\n21/21 [==============================] - 31s 2s/step - loss: 0.1376 - categorical_accuracy: 0.9894 - val_loss: 0.6962 - val_categorical_accuracy: 0.8000\n\nEpoch 00035: saving model to model_init_2022-02-2315_53_35.026342/model-00035-0.13759-0.98944-0.69623-0.80000.h5\nEpoch 36/75\n21/21 [==============================] - 30s 1s/step - loss: 0.1290 - categorical_accuracy: 0.9849 - val_loss: 0.8854 - val_categorical_accuracy: 0.7000\n\nEpoch 00036: saving model to model_init_2022-02-2315_53_35.026342/model-00036-0.12899-0.98492-0.88538-0.70000.h5\nEpoch 37/75\n21/21 [==============================] - 31s 2s/step - loss: 0.1921 - categorical_accuracy: 0.9638 - val_loss: 0.7163 - val_categorical_accuracy: 0.7600\n\nEpoch 00037: saving model to model_init_2022-02-2315_53_35.026342/model-00037-0.19215-0.96380-0.71630-0.76000.h5\nEpoch 38/75\n21/21 [==============================] - 31s 2s/step - loss: 0.1954 - categorical_accuracy: 0.9578 - val_loss: 0.5545 - val_categorical_accuracy: 0.8400\n\nEpoch 00038: saving model to model_init_2022-02-2315_53_35.026342/model-00038-0.19540-0.95777-0.55453-0.84000.h5\nEpoch 39/75\n21/21 [==============================] - 30s 2s/step - loss: 0.1622 - categorical_accuracy: 0.9638 - val_loss: 0.6205 - val_categorical_accuracy: 0.8100\n\nEpoch 00039: saving model to model_init_2022-02-2315_53_35.026342/model-00039-0.16222-0.96380-0.62052-0.81000.h5\nEpoch 40/75\n21/21 [==============================] - 29s 1s/step - loss: 0.1406 - categorical_accuracy: 0.9804 - val_loss: 0.7636 - val_categorical_accuracy: 0.7900\n\nEpoch 00040: saving model to model_init_2022-02-2315_53_35.026342/model-00040-0.14063-0.98039-0.76358-0.79000.h5\nEpoch 41/75\n21/21 [==============================] - 31s 2s/step - loss: 0.1281 - categorical_accuracy: 0.9744 - val_loss: 0.5098 - val_categorical_accuracy: 0.8300\n\nEpoch 00041: saving model to model_init_2022-02-2315_53_35.026342/model-00041-0.12808-0.97436-0.50983-0.83000.h5\nEpoch 42/75\n21/21 [==============================] - 30s 2s/step - loss: 0.1315 - categorical_accuracy: 0.9774 - val_loss: 0.5075 - val_categorical_accuracy: 0.8700\n\nEpoch 00042: saving model to model_init_2022-02-2315_53_35.026342/model-00042-0.13147-0.97738-0.50750-0.87000.h5\nEpoch 43/75\n21/21 [==============================] - 30s 2s/step - loss: 0.1106 - categorical_accuracy: 0.9774 - val_loss: 0.9177 - val_categorical_accuracy: 0.7100\n\nEpoch 00043: saving model to model_init_2022-02-2315_53_35.026342/model-00043-0.11064-0.97738-0.91771-0.71000.h5\nEpoch 44/75\n21/21 [==============================] - 29s 1s/step - loss: 0.1254 - categorical_accuracy: 0.9774 - val_loss: 0.5857 - val_categorical_accuracy: 0.8100\n\nEpoch 00044: saving model to model_init_2022-02-2315_53_35.026342/model-00044-0.12536-0.97738-0.58567-0.81000.h5\nEpoch 45/75\n21/21 [==============================] - 31s 2s/step - loss: 0.1090 - categorical_accuracy: 0.9819 - val_loss: 0.4150 - val_categorical_accuracy: 0.8700\n\nEpoch 00045: saving model to model_init_2022-02-2315_53_35.026342/model-00045-0.10904-0.98190-0.41503-0.87000.h5\nEpoch 46/75\n21/21 [==============================] - 30s 2s/step - loss: 0.0945 - categorical_accuracy: 0.9864 - val_loss: 1.1595 - val_categorical_accuracy: 0.6800\n\nEpoch 00046: saving model to model_init_2022-02-2315_53_35.026342/model-00046-0.09447-0.98643-1.15950-0.68000.h5\nEpoch 47/75\n21/21 [==============================] - 30s 2s/step - loss: 0.0787 - categorical_accuracy: 0.9940 - val_loss: 0.4727 - val_categorical_accuracy: 0.8500\n\nEpoch 00047: saving model to model_init_2022-02-2315_53_35.026342/model-00047-0.07866-0.99397-0.47269-0.85000.h5\nEpoch 48/75\n21/21 [==============================] - 29s 1s/step - loss: 0.0690 - categorical_accuracy: 0.9940 - val_loss: 0.5475 - val_categorical_accuracy: 0.7900\n\nEpoch 00048: saving model to model_init_2022-02-2315_53_35.026342/model-00048-0.06897-0.99397-0.54753-0.79000.h5\nEpoch 49/75\n21/21 [==============================] - 30s 2s/step - loss: 0.0606 - categorical_accuracy: 0.9970 - val_loss: 0.4649 - val_categorical_accuracy: 0.8500\n\nEpoch 00049: saving model to model_init_2022-02-2315_53_35.026342/model-00049-0.06063-0.99698-0.46486-0.85000.h5\nEpoch 50/75\n21/21 [==============================] - 30s 1s/step - loss: 0.0624 - categorical_accuracy: 0.9970 - val_loss: 0.5728 - val_categorical_accuracy: 0.8400\n\nEpoch 00050: saving model to model_init_2022-02-2315_53_35.026342/model-00050-0.06244-0.99698-0.57282-0.84000.h5\nEpoch 51/75\n21/21 [==============================] - 30s 2s/step - loss: 0.0561 - categorical_accuracy: 0.9970 - val_loss: 0.3231 - val_categorical_accuracy: 0.9000\n\nEpoch 00051: saving model to model_init_2022-02-2315_53_35.026342/model-00051-0.05608-0.99698-0.32312-0.90000.h5\nEpoch 52/75\n21/21 [==============================] - 29s 1s/step - loss: 0.0542 - categorical_accuracy: 0.9985 - val_loss: 0.5341 - val_categorical_accuracy: 0.8400\n\nEpoch 00052: saving model to model_init_2022-02-2315_53_35.026342/model-00052-0.05417-0.99849-0.53414-0.84000.h5\nEpoch 53/75\n21/21 [==============================] - 30s 2s/step - loss: 0.0786 - categorical_accuracy: 0.9925 - val_loss: 0.7116 - val_categorical_accuracy: 0.8100\n\nEpoch 00053: saving model to model_init_2022-02-2315_53_35.026342/model-00053-0.07860-0.99246-0.71156-0.81000.h5\nEpoch 54/75\n21/21 [==============================] - 30s 2s/step - loss: 0.0756 - categorical_accuracy: 0.9910 - val_loss: 0.5604 - val_categorical_accuracy: 0.8600\n\nEpoch 00054: saving model to model_init_2022-02-2315_53_35.026342/model-00054-0.07564-0.99095-0.56042-0.86000.h5\nEpoch 55/75\n21/21 [==============================] - 30s 2s/step - loss: 0.0503 - categorical_accuracy: 0.9970 - val_loss: 1.4660 - val_categorical_accuracy: 0.6300\n\nEpoch 00055: saving model to model_init_2022-02-2315_53_35.026342/model-00055-0.05029-0.99698-1.46599-0.63000.h5\nEpoch 56/75\n21/21 [==============================] - 29s 1s/step - loss: 0.0621 - categorical_accuracy: 0.9955 - val_loss: 0.6219 - val_categorical_accuracy: 0.8300\n\nEpoch 00056: saving model to model_init_2022-02-2315_53_35.026342/model-00056-0.06207-0.99548-0.62187-0.83000.h5\nEpoch 57/75\n21/21 [==============================] - 31s 2s/step - loss: 0.0406 - categorical_accuracy: 1.0000 - val_loss: 0.6298 - val_categorical_accuracy: 0.8300\n\nEpoch 00057: saving model to model_init_2022-02-2315_53_35.026342/model-00057-0.04056-1.00000-0.62977-0.83000.h5\nEpoch 58/75\n21/21 [==============================] - 30s 1s/step - loss: 0.0535 - categorical_accuracy: 0.9925 - val_loss: 0.6491 - val_categorical_accuracy: 0.8600\n\nEpoch 00058: saving model to model_init_2022-02-2315_53_35.026342/model-00058-0.05347-0.99246-0.64910-0.86000.h5\nEpoch 59/75\n21/21 [==============================] - 30s 2s/step - loss: 0.0863 - categorical_accuracy: 0.9834 - val_loss: 1.5311 - val_categorical_accuracy: 0.6500\n\nEpoch 00059: saving model to model_init_2022-02-2315_53_35.026342/model-00059-0.08631-0.98341-1.53107-0.65000.h5\nEpoch 60/75\n21/21 [==============================] - 29s 1s/step - loss: 0.1682 - categorical_accuracy: 0.9502 - val_loss: 1.5134 - val_categorical_accuracy: 0.6300\n\nEpoch 00060: saving model to model_init_2022-02-2315_53_35.026342/model-00060-0.16824-0.95023-1.51337-0.63000.h5\nEpoch 61/75\n21/21 [==============================] - 30s 1s/step - loss: 0.1808 - categorical_accuracy: 0.9517 - val_loss: 1.7442 - val_categorical_accuracy: 0.5100\n\nEpoch 00061: saving model to model_init_2022-02-2315_53_35.026342/model-00061-0.18084-0.95173-1.74415-0.51000.h5\nEpoch 62/75\n21/21 [==============================] - 30s 2s/step - loss: 0.1143 - categorical_accuracy: 0.9713 - val_loss: 0.4965 - val_categorical_accuracy: 0.8500\n\nEpoch 00062: saving model to model_init_2022-02-2315_53_35.026342/model-00062-0.11431-0.97134-0.49652-0.85000.h5\nEpoch 63/75\n21/21 [==============================] - 30s 2s/step - loss: 0.0615 - categorical_accuracy: 0.9955 - val_loss: 0.5390 - val_categorical_accuracy: 0.8700\n\nEpoch 00063: saving model to model_init_2022-02-2315_53_35.026342/model-00063-0.06149-0.99548-0.53900-0.87000.h5\nEpoch 64/75\n21/21 [==============================] - 29s 1s/step - loss: 0.0608 - categorical_accuracy: 0.9955 - val_loss: 0.5266 - val_categorical_accuracy: 0.8400\n\nEpoch 00064: saving model to model_init_2022-02-2315_53_35.026342/model-00064-0.06084-0.99548-0.52659-0.84000.h5\nEpoch 65/75\n21/21 [==============================] - 30s 2s/step - loss: 0.0490 - categorical_accuracy: 0.9985 - val_loss: 0.5193 - val_categorical_accuracy: 0.8500\n\nEpoch 00065: saving model to model_init_2022-02-2315_53_35.026342/model-00065-0.04897-0.99849-0.51931-0.85000.h5\nEpoch 66/75\n21/21 [==============================] - 30s 1s/step - loss: 0.0412 - categorical_accuracy: 1.0000 - val_loss: 0.6201 - val_categorical_accuracy: 0.8300\n\nEpoch 00066: saving model to model_init_2022-02-2315_53_35.026342/model-00066-0.04121-1.00000-0.62006-0.83000.h5\nEpoch 67/75\n21/21 [==============================] - 31s 2s/step - loss: 0.0485 - categorical_accuracy: 0.9940 - val_loss: 0.4389 - val_categorical_accuracy: 0.8900\n\nEpoch 00067: saving model to model_init_2022-02-2315_53_35.026342/model-00067-0.04849-0.99397-0.43887-0.89000.h5\nEpoch 68/75\n21/21 [==============================] - 29s 1s/step - loss: 0.0452 - categorical_accuracy: 0.9955 - val_loss: 0.4930 - val_categorical_accuracy: 0.9000\n\nEpoch 00068: saving model to model_init_2022-02-2315_53_35.026342/model-00068-0.04520-0.99548-0.49298-0.90000.h5\nEpoch 69/75\n21/21 [==============================] - 30s 2s/step - loss: 0.0419 - categorical_accuracy: 0.9985 - val_loss: 0.5045 - val_categorical_accuracy: 0.8700\n\nEpoch 00069: saving model to model_init_2022-02-2315_53_35.026342/model-00069-0.04190-0.99849-0.50449-0.87000.h5\nEpoch 70/75\n21/21 [==============================] - 30s 2s/step - loss: 0.0417 - categorical_accuracy: 0.9985 - val_loss: 0.6156 - val_categorical_accuracy: 0.8500\n\nEpoch 00070: saving model to model_init_2022-02-2315_53_35.026342/model-00070-0.04172-0.99849-0.61556-0.85000.h5\nEpoch 71/75\n21/21 [==============================] - 30s 2s/step - loss: 0.0420 - categorical_accuracy: 0.9955 - val_loss: 0.6816 - val_categorical_accuracy: 0.8300\n\nEpoch 00071: saving model to model_init_2022-02-2315_53_35.026342/model-00071-0.04200-0.99548-0.68162-0.83000.h5\nEpoch 72/75\n21/21 [==============================] - 29s 1s/step - loss: 0.0440 - categorical_accuracy: 0.9985 - val_loss: 0.4994 - val_categorical_accuracy: 0.8600\n\nEpoch 00072: saving model to model_init_2022-02-2315_53_35.026342/model-00072-0.04397-0.99849-0.49942-0.86000.h5\nEpoch 73/75\n21/21 [==============================] - 30s 1s/step - loss: 0.0386 - categorical_accuracy: 1.0000 - val_loss: 0.5312 - val_categorical_accuracy: 0.8500\n\nEpoch 00073: saving model to model_init_2022-02-2315_53_35.026342/model-00073-0.03859-1.00000-0.53123-0.85000.h5\nEpoch 74/75\n21/21 [==============================] - 31s 2s/step - loss: 0.0346 - categorical_accuracy: 1.0000 - val_loss: 0.4814 - val_categorical_accuracy: 0.8700\n\nEpoch 00074: saving model to model_init_2022-02-2315_53_35.026342/model-00074-0.03458-1.00000-0.48136-0.87000.h5\nEpoch 75/75\n21/21 [==============================] - 30s 2s/step - loss: 0.0392 - categorical_accuracy: 0.9985 - val_loss: 0.5518 - val_categorical_accuracy: 0.8700\n\nEpoch 00075: saving model to model_init_2022-02-2315_53_35.026342/model-00075-0.03924-0.99849-0.55176-0.87000.h5\n","output_type":"stream"},{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"<keras.callbacks.History at 0x7f343021c250>"},"metadata":{}}]},{"cell_type":"code","source":"# MAX VALIDATION SET ACCURACY ACHIEVED AT\n# Epoch 51/75\n# 21/21 [==============================] - 30s 2s/step - loss: 0.0561 - categorical_accuracy: 0.9970 - val_loss: 0.3231 - val_categorical_accuracy: 0.9000\n# Epoch 00051: saving model to model_init_2022-02-2315_53_35.026342/model-00051-0.05608-0.99698-0.32312-0.90000.h5","metadata":{},"execution_count":null,"outputs":[]}]}